services:
  # 1. MOTOR DE INFERENCIA
  vllm-engine:
    image: malkaf/vllm-blackwell-optimizer:latest
    container_name: vllm-engine
    runtime: nvidia
    privileged: true
    ipc: host
    cpuset: ${CPU_SET}
    shm_size: '64gb'
    ulimits:
      memlock: -1          # CRÃTICO: Permite bloquear RAM para DMA/offload
      stack: 67108864      # 64MB stack
    ports:
      - "8000:8000"
    volumes:
      - "${MODELS_PATH}:/model_dir"
      - "${HF_CACHE_PATH}:/root/.cache/huggingface"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - VLLM_CPU_KV_CACHE_PRECISION=bf16
      - VLLM_USE_V1=0
      - VLLM_FUSED_MOE_CHUNK_SIZE=32768
      - VLLM_NO_USAGE_STATS=1
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=${VLLM_ALLOW_LONG_MAX_MODEL_LEN}
      - NCCL_P2P_LEVEL=${NCCL_P2P_LEVEL}
      - VLLM_SKIP_P2P_CHECK=${VLLM_SKIP_P2P_CHECK}
      - NCCL_IGNORE_CPU_AFFINITY=${NCCL_IGNORE_CPU_AFFINITY}
      - NCCL_DMABUF_ENABLE=1
      - NCCL_SHM_DISABLE=1
      - VLLM_LOG_P2P_ACCESS_CHECK=0
      - OMP_NUM_THREADS=24   # Ajustado para Threadripper (1 CCD)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      --model /model_dir
      --served-model-name ${SERVED_MODEL_NAME}
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE}
      --max-model-len 524288
      --quantization ${QUANTIZATION}
      --dtype auto
      --kv-cache-dtype auto
      --gpu-memory-utilization 0.84
      --max-num-batched-tokens 16384
      --enforce-eager
      --distributed-executor-backend mp
      --disable-custom-all-reduce
      --enable-chunked-prefill
      --attention-backend FLASHINFER
      --max-num-seqs 1
      --cpu-offload-gb 20
      --hf-overrides '{"rope_scaling":{"rope_type":"yarn","factor":2.0,"original_max_position_embeddings":262144,"attention_factor":0.707,"beta_fast":32,"beta_slow":1},"rope_theta":100000000}'
      --enable-auto-tool-choice
      --tool-call-parser qwen3_coder
   
  # 2. PROXY LITELLM
  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    ports:
      - "4000:4000"
    volumes:
      - ./litellm-config.yaml:/app/config.yaml
    environment:
      - LITELLM_LOG=INFO
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=http://langfuse-bunker:3000
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
    command: [ "--config", "/app/config.yaml", "--port", "4000" ]
    depends_on:
      - vllm-engine

  # 3. BASE DE DATOS
  db-bunker:
    image: postgres:16
    container_name: db-bunker
    restart: always
    environment:
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # 4. OBSERVABILIDAD
  langfuse-bunker:
    image: langfuse/langfuse:2
    container_name: langfuse-bunker
    depends_on:
      - db-bunker
    ports:
      - "3000:3000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET}
      - SALT=${SALT}
      - NEXTAUTH_URL=${NEXTAUTH_URL}
      - LANGFUSE_INIT_PROJECT_NAME=BunkerAI
      - TELEMETRY_ENABLED=false

volumes:
  postgres_data:
    external: true
    name: sovereign-ai-stack_postgres_data
