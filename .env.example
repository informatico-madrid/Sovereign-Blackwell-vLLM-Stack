# === HOST PATHS ===
MODELS_ROOT=/path/to/your/models
MODELS_PATH=\${MODELS_ROOT}/qwen3-coder-30b-awq
HF_CACHE_PATH=/path/to/your/hf_cache
HF_TOKEN=your_huggingface_token_here

# === INFERENCE ENGINE (vLLM) ===
SERVED_MODEL_NAME=qwen3-coder-30b-awq
TENSOR_PARALLEL_SIZE=2
MAX_MODEL_LEN=262144
GPU_MEMORY_UTILIZATION=0.90
QUANTIZATION=compressed-tensors
CPU_SET=1-5,25-29

# NCCL & P2P Optimizations (Required for Blackwell/Kernel 6.14)
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
NCCL_P2P_LEVEL=PCI
VLLM_SKIP_P2P_CHECK=1
NCCL_IGNORE_CPU_AFFINITY=1

# === DATABASE (Postgres) ===
DB_USER=bunker_admin
DB_PASSWORD=your_secure_password
DB_NAME=langfuse
DATABASE_URL=postgresql://\${DB_USER}:\${DB_PASSWORD}@db-bunker:5432/\${DB_NAME}

# === OBSERVABILITY (Langfuse) ===
SALT=at_least_32_chars_random_string
LANGFUSE_PUBLIC_KEY=your_public_key
LANGFUSE_SECRET_KEY=your_secret_key
LANGFUSE_HOST=http://langfuse-bunker:3000
NEXTAUTH_SECRET=your_nextauth_secret
NEXTAUTH_URL=http://your_server_ip:3000

# === LITELLM ===
LITELLM_MASTER_KEY=your_master_key

