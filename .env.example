# ==============================================================================
# SOVEREIGN AI STACK - PRODUCTION CONFIGURATION (NATIVE 256K CONTEXT)
# Optimized for: Dual RTX 5090 (Blackwell) + AMD Threadripper
# ==============================================================================

# === HOST INFRASTRUCTURE PATHS ===
# Base directory where all your local models are stored
MODELS_ROOT=/mnt/bunker_data/ai/models
# Specific path to the model directory (e.g., Qwen3-Coder-30B-Instruct-FP8)
MODELS_PATH=${MODELS_ROOT}/qwen3-coder-30b-fp8
# Persistent cache for HuggingFace metadata and shards
HF_CACHE_PATH=/mnt/bunker_data/ai/hf_cache
# Your HuggingFace Token (Required for gated models)
HF_TOKEN=your_hf_token_here

# === INFERENCE ENGINE (vLLM) ===
# Name exposed via the OpenAI-compatible API
SERVED_MODEL_NAME=qwen3-coder-30b-fp8
# Number of GPUs to shard the model (TP=2 for Dual GPU setup)
TENSOR_PARALLEL_SIZE=2
# Native context window. 262144 is the native limit for Qwen3-Coder. 
# Avoid values >262144 without YaRN scaling to prevent numerical instability.
MAX_MODEL_LEN=262144
# VRAM reservation fraction (0.84). 
# Provides a balanced headroom for activation tensors on Blackwell architecture.
GPU_MEMORY_UTILIZATION=0.84
# Weight precision. 'fp8' is highly optimized for H100/RTX 50-series Tensor Cores.
QUANTIZATION=fp8
# CPU core pinning (e.g., mapping to a specific Threadripper CCD)
# Prevents context switching and reduces latency.
CPU_SET=1-5,25-29

# === LOW-LEVEL OPTIMIZATIONS (Blackwell & NCCL) ===
# Enable vLLM V1 high-performance engine for Blackwell-specific kernels
VLLM_USE_V1=1
# Set to 0 for strict stability. Prevents model from exceeding native context limit.
VLLM_ALLOW_LONG_MAX_MODEL_LEN=0
# Peer-to-Peer communication level (PCI/NVL). Forces PCIe path for dual 5090.
NCCL_P2P_LEVEL=PCI
# Bypasses P2P accessibility checks to speed up engine initialization
VLLM_SKIP_P2P_CHECK=1
# Prevents NCCL from overriding Docker's CPU affinity settings (CPU_SET)
NCCL_IGNORE_CPU_AFFINITY=1

# === DATABASE (PostgreSQL) ===
# Credentials for the internal database container
DB_USER=admin
DB_PASSWORD=secure_admin_password
DB_NAME=langfuse
# Internal connection string for Langfuse to DB communication
DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@db-bunker:5432/${DB_NAME}

# === OBSERVABILITY (Langfuse) ===
# Security salt for encryption (Generate with: openssl rand -base64 32)
SALT=replace_with_32_character_random_string
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key
# Endpoint where Langfuse UI is exposed (Local IP or DNS)
LANGFUSE_HOST=http://192.168.1.XXX:3000
# Authentication secret and public URL for the observability dashboard
NEXTAUTH_SECRET=your_auth_secret_here
NEXTAUTH_URL=http://192.168.1.XXX:3000

# === PROXY GATEWAY (LiteLLM) ===
# Master API key to access the unified endpoint
LITELLM_MASTER_KEY=sk-master-2026